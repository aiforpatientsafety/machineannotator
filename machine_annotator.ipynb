{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "This is a machine annotator for Japanese free text incident reports of medication errors. This pipeline has been tested to be stable and works best on Google Colab. The machine annotator utilizes CUDA, so please set the Runtime type to 'GPU'.\n",
        "\n",
        "### SETUP:\n",
        "(1) Please mount the shared files (including \"wiki-ja.model\", \"config.json\", \"model_entity_220309.bin\", and \"model_3_220310_2.bin\") accordingly into \"TOKENIZER_MODEL\", \"BERT_CONIFG_FILE\", \"BERT_PRETRAINED_MODEL\", \"MODEL_SAVE_PATH\".\n",
        "\n",
        "(2) Next please set up input/output file paths:\n",
        "  - \"in_dir\": the input file path, it should be a xlsx file with two columns: \"id\" and \"report\", saved in the \"freetext\" sheet. \n",
        "  - \"out_dir\": the output file path, it will generate the entity-level predicted output from the trained machine-annotator.\n",
        "\n",
        "### RUN:\n",
        "Please run the code chunk by chunk and you will find the entity-level annotation in \"out_dir\".\n",
        "\n",
        "For any inquiries, please email to Dr Zoie SY Wong (zoiesywong@gmail.com)"
      ],
      "metadata": {
        "id": "WOpl9xT9CFYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and importing libraries"
      ],
      "metadata": {
        "collapsed": false,
        "id": "N2ewydD6vES-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: demoji in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neologdn in /usr/local/lib/python3.10/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install demoji\n",
        "!pip install neologdn"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAmlUmDUvETO",
        "outputId": "8aabea81-d06a-4c36-8b9a-7f493e6d32aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "uKHUYifrvETU"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: US-ASCII -*-\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertConfig, AdamW, BertModel\n",
        "import re\n",
        "import neologdn\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/mnt; to attempt to forcibly remount, call drive.mount(\"/content/mnt\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqlqdPLvETa",
        "outputId": "f57473cc-1fe4-45a1-b56f-11183c02eda4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": [
        "in_dir = '/content/mnt/MyDrive/program/in/freetext.xlsx'\n",
        "out_dir = '/content/mnt/MyDrive/program/out/annotation.xlsx'\n",
        "TOKENIZER_MODEL = '/content/mnt/MyDrive/program/tokenizer/wiki-ja.model'\n",
        "BERT_CONIFG_FILE = '/content/mnt/MyDrive/program/entity_model/config.json'\n",
        "BERT_PRETRAINED_MODEL = '/content/mnt/MyDrive/program/entity_model/model_entity_220309.bin'\n",
        "MODEL_SAVE_PATH = '/content/mnt/MyDrive/program/trained_model/model_3_220310_2.bin'"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QDmofJ5jvETc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TbKS_Dj7vETe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "URL = re.compile(\n",
        "    r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\"\n",
        "    + r\"[a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.\"\n",
        "    + r\"[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.\"\n",
        "    + r\"[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.\"\n",
        "    + r\"[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
        ")\n",
        "MULTI_SPACE = re.compile(r\"[\\t ]+\")\n",
        "MULTI_LINE = re.compile(r\"\\n\\s+\")\n",
        "LINE = re.compile(r\"\\n\")\n",
        "\n",
        "def clean_basic(text):\n",
        "    text = text.lower()\n",
        "    # remove urls\n",
        "    text = URL.sub(\" \", text)\n",
        "    # replace multispaces to single\n",
        "    text = MULTI_SPACE.sub(\" \", text)\n",
        "    text = MULTI_LINE.sub(\"\\n\", text)\n",
        "    text = LINE.sub(\"?\", text)\n",
        "    # remove leading, trailing spaces\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%% Preprocessing\n"
        },
        "id": "cHjdnW9UvETg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "AsZzsJH8vETk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-5\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 8\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_STEPS = 0\n",
        "EPOCHS = 20\n",
        "EVAL_STEP = 50\n",
        "ACCUMULATION_STEP = 1\n",
        "DROPOUT_RATE = 0.3\n",
        "DEVICE = \"cuda\"\n",
        "POS_NUM_LABELS = 28\n",
        "INT_NUM_LABELS = 5\n",
        "REL_NUM_LABELS = 4\n",
        "\n",
        "mapping = {\n",
        "    'PAD': 0,\n",
        "    'O': 1,\n",
        "    'B-Date': 2,\n",
        "    'I-Date': 3,\n",
        "    'B-Dosage': 4,\n",
        "    'I-Dosage': 5,\n",
        "    'B-Drug': 6,\n",
        "    'I-Drug': 7,\n",
        "    'B-Duration': 8,\n",
        "    'I-Duration': 9,\n",
        "    'B-Form_form': 10,\n",
        "    'I-Form_form': 11,\n",
        "    'B-Form_mode': 12,\n",
        "    'I-Form_mode': 13,\n",
        "    'B-Frequency': 14,\n",
        "    'I-Frequency': 15,\n",
        "    'B-Route': 16,\n",
        "    'I-Route': 17,\n",
        "    'B-Strength_amount': 18,\n",
        "    'I-Strength_amount': 19,\n",
        "    'B-Strength_concentration': 20,\n",
        "    'I-Strength_concentration': 21,\n",
        "    'B-Strength_rate': 22,\n",
        "    'I-Strength_rate': 23,\n",
        "    'B-Timing': 24,\n",
        "    'I-Timing': 25,\n",
        "    'B-Wrong_patient': 26,\n",
        "    'I-Wrong_patient': 27,\n",
        "}\n",
        "\n",
        "ids_to_labels = dict(zip(list(mapping.values()), list(mapping.keys())))\n",
        "ids_to_labels_int = {1: 'O', 2: 'IA', 3: 'IN', 4: 'NA', 0: 'PAD'}\n",
        "ids_to_labels_rel = {1: 'O', 2: '1', 3: '2', 0: 'PAD'}"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Rtd6x6l5vETn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": [
        "class IncidentDataset(Dataset):\n",
        "    def __init__(self, texts, labels_pos, labels_int, labels_rel, tokenizer, max_len, eos_token, pad_token):\n",
        "\n",
        "        self.tokens, self.mask, self.id, self.labels_pos, self.labels_int, self.labels_rel = self.process_text(texts,\n",
        "                                                                                                               labels_pos,\n",
        "                                                                                                               labels_int,\n",
        "                                                                                                               labels_rel,\n",
        "                                                                                                               tokenizer,\n",
        "                                                                                                               max_len,\n",
        "                                                                                                               eos_token,\n",
        "                                                                                                               pad_token)\n",
        "\n",
        "    def process_text(self, data, labels_pos, labels_int, labels_rel, tokenizer, max_len, eos_token, pad_token):\n",
        "        tokenized_text = tokenizer.encode(data)\n",
        "\n",
        "        for i, tokens in enumerate(tokenized_text):\n",
        "            if len(tokens) >= max_len:\n",
        "                tokens = tokens[:max_len]\n",
        "                tokens[-1] = eos_token\n",
        "            else:\n",
        "                tokens.append(eos_token)\n",
        "                n = max_len - len(tokens)\n",
        "                paddings = [pad_token] * n\n",
        "                tokens.extend(paddings)\n",
        "\n",
        "            tokenized_text[i] = tokens\n",
        "\n",
        "        attention_mask = np.ones((len(tokenized_text), max_len))\n",
        "\n",
        "        attention_mask[np.array(tokenized_text) == 1] = 0\n",
        "        token_type_ids = np.zeros((len(tokenized_text), max_len))\n",
        "\n",
        "        for i, label in enumerate(labels_pos):\n",
        "            if len(label) >= max_len:\n",
        "                label = label[:max_len]\n",
        "                label[-1] = 0\n",
        "            else:\n",
        "                # label.append(0)\n",
        "                n = max_len - len(label)\n",
        "                paddings = [0] * n\n",
        "                label.extend(paddings)\n",
        "\n",
        "            labels_pos[i] = label\n",
        "\n",
        "        for i, label in enumerate(labels_int):\n",
        "            if len(label) >= max_len:\n",
        "                label = label[:max_len]\n",
        "                label[-1] = 0\n",
        "            else:\n",
        "                # label.append(0)\n",
        "                n = max_len - len(label)\n",
        "                paddings = [0] * n\n",
        "                label.extend(paddings)\n",
        "\n",
        "            labels_int[i] = label\n",
        "\n",
        "        for i, label in enumerate(labels_rel):\n",
        "            if len(label) >= max_len:\n",
        "                label = label[:max_len]\n",
        "                label[-1] = 0\n",
        "            else:\n",
        "                # label.append(0)\n",
        "                n = max_len - len(label)\n",
        "                paddings = [0] * n\n",
        "                label.extend(paddings)\n",
        "\n",
        "            labels_rel[i] = label\n",
        "\n",
        "        return tokenized_text, attention_mask, token_type_ids, labels_pos, labels_int, labels_rel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(self.tokens[item], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(self.mask[item], dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(self.id[item], dtype=torch.long),\n",
        "            \"label_pos\": torch.tensor(self.labels_pos[item], dtype=torch.long),\n",
        "            \"label_int\": torch.tensor(self.labels_int[item], dtype=torch.long),\n",
        "            \"label_rel\": torch.tensor(self.labels_rel[item], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class IncidentBert(nn.Module):\n",
        "    def __init__(self, BERT_CONIFG_FILE, reinit_n_layers=0):\n",
        "        super().__init__()\n",
        "        config = BertConfig.from_pretrained(BERT_CONIFG_FILE)\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.fc = nn.Linear(768, POS_NUM_LABELS)\n",
        "\n",
        "        self._init_weights(self.fc)\n",
        "\n",
        "        self.reinit_n_layers = reinit_n_layers\n",
        "        if reinit_n_layers > 0:\n",
        "            self._do_reinit()\n",
        "\n",
        "    def _do_reinit(self):\n",
        "        for n in range(self.reinit_n_layers):\n",
        "            self.bert.encoder.layer[-(n+1)].apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "\n",
        "        bert_out = self.bert(input_ids=input_ids,\n",
        "                             attention_mask=attention_mask,\n",
        "                             token_type_ids=token_type_ids)\n",
        "\n",
        "        output = bert_out.last_hidden_state\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class IncidentModel(nn.Module):\n",
        "    def __init__(self, BERT_CONIFG_FILE, BERT_PRETRAINED_MODEL, reinit_n_layers=0):\n",
        "        super().__init__()\n",
        "        self.bert = IncidentBert(BERT_CONIFG_FILE)\n",
        "        print(\"Loading weights\")\n",
        "        self.bert.load_state_dict(torch.load(BERT_PRETRAINED_MODEL))\n",
        "        print(\"Loaded weights\")\n",
        "\n",
        "        self.fc_int = nn.Linear(POS_NUM_LABELS, INT_NUM_LABELS)\n",
        "        self.fc_rel = nn.Linear(POS_NUM_LABELS, REL_NUM_LABELS)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "\n",
        "        bert_out = self.bert(input_ids=input_ids,\n",
        "                             attention_mask=attention_mask,\n",
        "                             token_type_ids=token_type_ids)\n",
        "\n",
        "        output_int = self.fc_int(bert_out)\n",
        "        output_rel = self.fc_rel(bert_out)\n",
        "\n",
        "        return bert_out, output_int, output_rel\n",
        "\n",
        "\n",
        "def loss_fn_pos(output, target, mask):\n",
        "    lfn = nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, POS_NUM_LABELS)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_fn_int(output, target, mask):\n",
        "    weights = [0.0001, 0.001, 0.6, 0.9, 0.9]\n",
        "    class_weights = torch.FloatTensor(weights).cuda()\n",
        "    lfn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, INT_NUM_LABELS)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_fn_rel(output, target, mask):\n",
        "    weights = [0.0001, 0.001, 0.1, 0.9]\n",
        "    class_weights = torch.FloatTensor(weights).cuda()\n",
        "    lfn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, REL_NUM_LABELS)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def create_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())\n",
        "\n",
        "    parameters = []\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(named_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else WEIGHT_DECAY\n",
        "\n",
        "        lr = LEARNING_RATE\n",
        "\n",
        "        if layer_num >= 69:\n",
        "            lr = LEARNING_RATE * 2\n",
        "\n",
        "        if layer_num >= 133:\n",
        "            lr = LEARNING_RATE * 5\n",
        "\n",
        "        if layer_num >= 192:\n",
        "            lr = LEARNING_RATE * 10\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fXsus8JFvETq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict from AI model\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JtK9K-UWvETt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "def report_predict(in_dir, out_dir, TOKENIZER_MODEL, BERT_CONIFG_FILE, BERT_PRETRAINED_MODEL, MODEL_SAVE_PATH):\n",
        "    tokenizer = spm.SentencePieceProcessor(TOKENIZER_MODEL)\n",
        "    model = IncidentModel(BERT_CONIFG_FILE, BERT_PRETRAINED_MODEL,)\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    newdf = pd.read_excel(in_dir, sheet_name='freetext')\n",
        "\n",
        "    newdf[\"report\"] = newdf[\"report\"].apply(lambda x: neologdn.normalize(x))\n",
        "    text = newdf.report.tolist()\n",
        "    cleaned_text = [clean_basic(i) for i in text]\n",
        "    newdf.report = cleaned_text\n",
        "\n",
        "    texts = newdf.report.apply(str).tolist()\n",
        "    tokenized_texts = []\n",
        "    labels_pos = []\n",
        "    labels_int = []\n",
        "    labels_rel = []\n",
        "\n",
        "    for i in range(len(newdf)):\n",
        "        tokenized_text = tokenizer.encode(newdf.report[i])\n",
        "        t = len(tokenized_text)\n",
        "        tokenized_texts.append(tokenized_text)\n",
        "        labels_pos.append([1]*t)\n",
        "        labels_int.append([1]*t)\n",
        "        labels_rel.append([1]*t)\n",
        "\n",
        "    texts = np.array(texts)\n",
        "    labels_pos = np.array(labels_pos)\n",
        "    labels_int = np.array(labels_int)\n",
        "    labels_rel = np.array(labels_rel)\n",
        "\n",
        "    dataset = IncidentDataset(texts.tolist(), labels_pos.tolist(), labels_int.tolist(), labels_rel.tolist(), tokenizer, MAX_LEN, 2, 1)\n",
        "    dataloader = DataLoader(dataset, batch_size=1, drop_last=False, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    outs_pos, outs_int, outs_rel = [], [], []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        types = batch[\"token_type_ids\"].to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_pos, out_int, out_rel = model(input_ids=ids, attention_mask=mask, token_type_ids=types)\n",
        "\n",
        "        out_pos = torch.argmax(out_pos, dim=-1)\n",
        "        out_int = torch.argmax(out_int, dim=-1)\n",
        "        out_rel = torch.argmax(out_rel, dim=-1)\n",
        "\n",
        "        for s in range(out_pos.shape[0]):\n",
        "            pos = torch.masked_select(out_pos[s], mask[s] > 0)\n",
        "            pos = pos.cpu().numpy().tolist()[:-1]\n",
        "            pos = [ids_to_labels[id] for id in pos]\n",
        "            outs_pos.append(pos)\n",
        "\n",
        "            intention = torch.masked_select(out_int[s], mask[s] > 0)\n",
        "            intention = intention.cpu().numpy().tolist()[:-1]\n",
        "            intention = [ids_to_labels_int[id] for id in intention]\n",
        "            outs_int.append(intention)\n",
        "\n",
        "            rel = torch.masked_select(out_rel[s], mask[s] > 0)\n",
        "            rel = rel.cpu().numpy().tolist()[:-1]\n",
        "            rel = [ids_to_labels_rel[id] for id in rel]\n",
        "            outs_rel.append(rel)\n",
        "\n",
        "    tokenized_texts=[]\n",
        "    for i in range(len(texts.tolist())):\n",
        "        tokenized_texts.append(tokenizer.encode(texts.tolist()[i], out_type=str))\n",
        "    ss = pd.DataFrame({\n",
        "        \"report\":texts.tolist(),\n",
        "        \"tokenized_report\": tokenized_texts,\n",
        "        \"outs_pos\": outs_pos,\n",
        "        \"outs_int\": outs_int,\n",
        "        \"outs_rel\": outs_rel})\n",
        "\n",
        "    ss['id'] = newdf.id\n",
        "    ss.to_excel(out_dir,index=False)\n",
        "\n",
        "    return out_dir"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DkuLrZFLvETu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing - incident type prediction"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "QeJdcfVcvETw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": [
        "def drug_incidents(data):\n",
        "    drug_entities = data[data[\"entity\"] == \"Drug\"]\n",
        "    intentions = data[\"label\"].values.tolist()\n",
        "    if \"IN\" in intentions:\n",
        "        if \"NA\" in intentions:\n",
        "            return [\"Wrong Drug\"]\n",
        "        else:\n",
        "            return [\"Drug Omission\"]\n",
        "    if \"NA\" in intentions:\n",
        "        return [\"Extra Drug\"]\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def other_incidents(data, entity_type):\n",
        "    entities = data[((data[\"entity\"] == entity_type) & ((data[\"label\"] == \"IN\") | (data[\"label\"] == \"NA\")))]\n",
        "    if entities.shape[0] > 0:\n",
        "        return [\"Wrong \" + entity_type]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "\n",
        "def find_incidents(data):\n",
        "    incidents = []\n",
        "    # incidents += drug_incidents(data)\n",
        "    for i in [\n",
        "        \"Strength_amount\",\n",
        "        \"Strength_rate\",\n",
        "        \"Strength_concentration\",\n",
        "        \"Dosage\",\n",
        "        \"Form_form\",\n",
        "        \"Form_mode\",\n",
        "        \"Route\",\n",
        "        \"Date\",\n",
        "        \"Duration\",\n",
        "        \"Timing\",\n",
        "        \"Frequency\",\n",
        "        \"Drug\"\n",
        "    ]:\n",
        "        incidents += other_incidents(data, i)\n",
        "\n",
        "    if len(incidents) == 0:\n",
        "        return \"Others\"\n",
        "    else:\n",
        "        return \",\".join(incidents)\n",
        "\n",
        "\n",
        "def entity_results(in_dir, out_dir):\n",
        "    # path to the file/output of model\n",
        "    ss = pd.read_excel(in_dir, index_col=0)\n",
        "    ss = ss.reset_index()\n",
        "\n",
        "    # convert list input to string in dataframe\n",
        "    ss.outs_pos = ss.outs_pos.apply(eval)\n",
        "    ss.outs_int = ss.outs_int.apply(eval)\n",
        "    ss.outs_rel = ss.outs_rel.apply(eval)\n",
        "    ss.tokenized_report = ss.tokenized_report.apply(eval)\n",
        "\n",
        "    df_total = pd.DataFrame()\n",
        "    for idx, row in tqdm(ss.loc[:].iterrows()):\n",
        "\n",
        "        ans = []\n",
        "        for idx2, ele in enumerate(row.outs_pos):\n",
        "\n",
        "            mini = []\n",
        "            if ele.startswith(\"B\"):\n",
        "                count = idx2\n",
        "                while count <= len(row.outs_pos) - 2:\n",
        "                    mini.append(count)\n",
        "                    count += 1\n",
        "                    if row.outs_pos[count] == \"O\" or row.outs_pos[count].startswith(\"B\"):\n",
        "                        break\n",
        "            if mini != []:\n",
        "                ans.append(mini)\n",
        "        df_sub = pd.DataFrame(\n",
        "            columns=[\"id\", \"reports\", \"entity_name\", \"start_idx\", \"end_idx\", \"entity\", \"label\", \"rel_index\"])\n",
        "        final_ans = []\n",
        "        for a in ans:\n",
        "            entity = \"\".join(row.tokenized_report[a[0]:a[-1] + 1])\n",
        "\n",
        "            ent_e = row.outs_pos[a[0]:a[-1] + 1]\n",
        "            ent_e = [ele.replace(\"I-\", \"\") for ele in ent_e]\n",
        "            ent_e = [ele.replace(\"B-\", \"\") for ele in ent_e]\n",
        "            ent_e = list(set(ent_e))\n",
        "\n",
        "            for ele in reversed(ent_e):\n",
        "                if ele in [\"O\", \"<pad>\"]:\n",
        "                    ent_e.remove(ele)\n",
        "            if len(ent_e) > 0:\n",
        "                ent_ele = ent_e[0]\n",
        "            else:\n",
        "                ent_ele = None\n",
        "\n",
        "            int_f = row.outs_int[a[0]:a[-1] + 1]\n",
        "            int_f = list(set(int_f))\n",
        "            for ele in reversed(int_f):\n",
        "                if ele in [\"O\", \"<pad>\"]:\n",
        "                    int_f.remove(ele)\n",
        "            if len(int_f) > 0:\n",
        "                int_fac_ele = int_f[0]\n",
        "            else:\n",
        "                int_fac_ele = None\n",
        "\n",
        "            rel_a = row.outs_rel[a[0]:a[-1] + 1]\n",
        "\n",
        "            rel_a = list(set(rel_a))\n",
        "            for ele in reversed(rel_a):\n",
        "                if ele in [\"O\", \"<pad>\"]:\n",
        "                    rel_a.remove(ele)\n",
        "            if len(rel_a) > 0:\n",
        "                rel_a_ele = rel_a[0]\n",
        "            else:\n",
        "                rel_a_ele = None\n",
        "            if len(row.tokenized_report) > 1:\n",
        "                test = \"\".join(list(row.tokenized_report[0].replace(\"?\", \"\")) + list(row.tokenized_report[1:a[0]]))\n",
        "            else:\n",
        "                print(\"error_occured\")\n",
        "\n",
        "            # start_idx = len(test)+1\n",
        "            start_idx = len(test)\n",
        "            end_idx = start_idx + len(entity)\n",
        "            new_row = [row.id, row.report, entity, start_idx, end_idx, ent_ele, int_fac_ele, rel_a_ele]\n",
        "            df_sub.loc[df_sub.shape[0]] = new_row\n",
        "        df_total = pd.concat([df_total, df_sub])\n",
        "\n",
        "    df_total.to_excel(out_dir, index=False)\n",
        "\n",
        "    df_total = pd.read_excel(out_dir, engine='openpyxl', keep_default_na=False)\n",
        "    output_df = pd.DataFrame(columns=list(df_total.columns) + [\"incident_types\"])\n",
        "    groups = df_total.groupby([\"id\", \"reports\"]).groups\n",
        "\n",
        "    for group in groups.items():\n",
        "        report_id = group[0][0]\n",
        "        entities = df_total.iloc[list(group[1])].sort_values(by='start_idx')\n",
        "        entities[\"incident_types\"] = find_incidents(entities)\n",
        "        output_df = output_df.append(entities, ignore_index=True)\n",
        "\n",
        "    output_df.to_excel(out_dir)\n",
        "\n",
        "    return out_dir\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%% #### Post processing\n"
        },
        "id": "a2KvxOKAvETx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output annotations"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cHxR3SepvET2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights\n",
            "Loaded weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-114c2873df29>:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  labels_pos = np.array(labels_pos)\n",
            "<ipython-input-20-114c2873df29>:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  labels_int = np.array(labels_int)\n",
            "<ipython-input-20-114c2873df29>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  labels_rel = np.array(labels_rel)\n",
            "4it [00:00, 79.75it/s]\n",
            "<ipython-input-21-83c215b6454b>:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  output_df = output_df.append(entities, ignore_index=True)\n",
            "<ipython-input-21-83c215b6454b>:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  output_df = output_df.append(entities, ignore_index=True)\n",
            "<ipython-input-21-83c215b6454b>:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  output_df = output_df.append(entities, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity Level Output from AI Model: /content/mnt/MyDrive/program/out/annotation.xlsx\n"
          ]
        }
      ],
      "source": [
        "sent_out_dir = out_dir.replace('.xlsx','-predict.xlsx')\n",
        "sent_out_dir = report_predict(in_dir, sent_out_dir,TOKENIZER_MODEL, BERT_CONIFG_FILE, BERT_PRETRAINED_MODEL, MODEL_SAVE_PATH)\n",
        "out_dir = entity_results(in_dir = sent_out_dir, out_dir=out_dir)\n",
        "print('Entity Level Output from AI Model:', out_dir)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jW4DciNvET3",
        "outputId": "73c4980f-57c8-4050-bdd2-2e9c68f72af4"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}